{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T12:15:41.970605Z",
     "start_time": "2021-03-14T12:15:39.311645Z"
    }
   },
   "outputs": [],
   "source": [
    "## Libraries for file management\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "## Libraries for data management\n",
    "import numpy as np\n",
    "\n",
    "## Libraries for website interactions\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def soupify(url):\n",
    "    \"\"\"Convert a html file via url into bs4 soup with urlib3 and bs4.\"\"\"\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', url)\n",
    "    html_doc = r.data\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def processing(text):\n",
    "    \"\"\"Tokenize a given string and create a dictionary. Return stats such as\n",
    "    number of words, size of the dictionary, number of sentences. Also return\n",
    "    the dictionary.\"\"\"\n",
    "    \n",
    "    ## Tokenizing the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    ## Creating a dictionary of the token occurrences\n",
    "    dictionary = {}\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            dictionary[word] += 1\n",
    "        else:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    ## Creating a list of sentences\n",
    "    sentences = text.split(\". \")\n",
    "    \n",
    "    return len(words), len(dictionary), len(sentences), dictionary\n",
    "\n",
    "\n",
    "def extract(soup):\n",
    "    \"\"\"Extracts the text, title, date and categories of the article and then\n",
    "    dumps it into the database stored in pickle file (overwriting the old version).\"\"\"\n",
    "    \n",
    "    passages = soup.find_all(\"p\",\"text_3v_J6Y0G\")\n",
    "    text = \" \".join([passage.text for passage in passages])\n",
    "    text = text.replace('\"','').replace(\"'\",\"\")\n",
    "    \n",
    "    date = soup.find_all(\"time\")[-1]['datetime']\n",
    "    categories = [x.text for x in soup.find_all(\"a\", class_=\"link_2imnEnEf\")]\n",
    "    \n",
    "    title = soup.find(\"h1\",\"title_iP7Q1aiP\").text\n",
    "    for x in [\".\",\",\",\":\",\"/\",\"â€¢\",\"'\",'\"',\"?\",\"*\"]:\n",
    "        title = title.replace(x,\"\")\n",
    "    title = title.strip()\n",
    "    \n",
    "    w,d,s,dictionary = processing(text)\n",
    "    \n",
    "    if len(passages) != 0:\n",
    "        arr = np.array([text, categories, dictionary, w, d, s])\n",
    "        \n",
    "        with open('article_database.pickle', 'rb+') as handle:\n",
    "            db = pickle.load(handle)\n",
    "            db[f\"{date[:10]}_{title}\"] = arr\n",
    "            pickle.dump(db, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def scrape(link_categories):\n",
    "    \"\"\"Scrape the articles from a given list of categories on the NOS website.\"\"\"\n",
    "    \n",
    "    n_new_arcticles = 0\n",
    "    \n",
    "    print(\"|\",end=\"\")\n",
    "    for category in link_categories:\n",
    "        ## Parse each category page\n",
    "        soup = soupify(f\"https://nos.nl/nieuws/{category}/\")\n",
    "        \n",
    "        ## Find all of the articles and remove all liveblogs\n",
    "        article_blocks = soup.find_all(\"a\", class_=\"link-block list-items__link\")\n",
    "        article_links = [f\"https://nos.nl{article['href']}\" for article in article_blocks if \"liveblog\" not in article['href']]\n",
    "        \n",
    "        \n",
    "        for link in article_links:\n",
    "            soup = soupify(link)\n",
    "            n_new_arcticles += extract(soup)\n",
    "        print(f\" {category} |\",end=\"\")\n",
    "        \n",
    "        handle = open('article_database.pickle', 'rb')\n",
    "        db = pickle.load(handle)\n",
    "        handle.close()\n",
    "    \n",
    "    return n_new_arcticles, len(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:08:57.193939Z",
     "start_time": "2021-03-14T13:08:55.290019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3481\n"
     ]
    }
   ],
   "source": [
    "# path = \".\\\\articles\"\n",
    "# articles = listdir(path)\n",
    "# articles.remove(\".ipynb_checkpoints\")\n",
    "\n",
    "# database = {}\n",
    "\n",
    "# for article in articles:\n",
    "#     contents = np.load(f\"{path}\\\\{article}\", allow_pickle=True)\n",
    "#     database[article[:-4]] = contents\n",
    "\n",
    "# print(len(database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T13:08:59.633238Z",
     "start_time": "2021-03-14T13:08:58.915000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3481\n"
     ]
    }
   ],
   "source": [
    "# with open('article_database.pickle', 'wb') as handle:\n",
    "#     pickle.dump(database, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('article_database.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "# b[\"2014-05-07_Ontvoeren schoolkinderen is in Nigeria inmiddels een winstgevende onderneming\"]\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\marco\\Documents\\GitHub\\NLP-exploration-with-NOS-articles\\extractor.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
