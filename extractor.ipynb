{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T14:06:05.334284Z",
     "start_time": "2020-11-19T14:06:03.614570Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def soupify(url):\n",
    "    \"\"\"Convert a html file via url into bs4 soup with urlib3 and bs4.\"\"\"\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', url)\n",
    "    html_doc = r.data\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def processing(text:str, verbose=False):\n",
    "    \"\"\"Tokenize a given string and create a dictionary. Return stats such as\n",
    "    number of words, size of the dictionary, number of sentences. Also return\n",
    "    the dictionary.\"\"\"\n",
    "    \n",
    "    ## Tokenizing the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    if verbose: print(f\"Number of words: {len(words)}\")\n",
    "\n",
    "    ## Creating a dictionary of the token occurrences\n",
    "    dictionary = {}\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            dictionary[word] += 1\n",
    "        else:\n",
    "            dictionary[word] = 1\n",
    "    if verbose: print(f\"Length of Dictionary: {len(dictionary)}\")\n",
    "\n",
    "    ## Creating a list of sentences\n",
    "    sentences = text.split(\". \")\n",
    "    if verbose: print(f\"Number of sentences: {len(sentences)}\")\n",
    "    \n",
    "    return len(words), len(dictionary), len(sentences), dictionary\n",
    "\n",
    "\n",
    "def extract(soup, save_file=False, verbose=False):\n",
    "    \"\"\"Extract the text, title, date and categories of the article. Optionally save\n",
    "    the file.\"\"\"\n",
    "    \n",
    "    passages = soup.find_all(\"p\",\"text_3v_J6Y0G\")\n",
    "    text = \" \".join([passage.text for passage in passages])\n",
    "    text = text.replace('\"','').replace(\"'\",\"\")\n",
    "    \n",
    "    date = soup.find_all(\"time\")[-1]['datetime']\n",
    "    categories = [x.text for x in soup.find_all(\"a\", class_=\"link_2imnEnEf\")]\n",
    "    \n",
    "    title = soup.find(\"h1\",\"title_iP7Q1aiP\").text\n",
    "    for x in [\".\",\",\",\":\",\"/\",\"â€¢\",\"'\",'\"',\"?\",\"*\"]:\n",
    "        title = title.replace(x,\"\")\n",
    "    title = title.strip()\n",
    "    \n",
    "    w,d,s,dictionary = processing(text)\n",
    "\n",
    "    if save_file:\n",
    "        if len(passages) != 0:\n",
    "            arr = np.array([text, categories, dictionary, w, d, s])\n",
    "            np.save(f\"articles\\\\{date[:10]}_{title}\", arr)\n",
    "            if verbose: print(f\"Saved file under 'articles\\\\{date[:10]}_{title}.txt'\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape(link_categories):\n",
    "    \"\"\"Text\"\"\"\n",
    "    \n",
    "    for category in link_categories:\n",
    "        ## Parse each category page\n",
    "        soup = soupify(f\"https://nos.nl/nieuws/{category}/\")\n",
    "        \n",
    "        ## Find all of the articles and remove all liveblogs\n",
    "        article_blocks = soup.find_all(\"a\", class_=\"link-block list-items__link\")\n",
    "        article_links = [f\"https://nos.nl{article['href']}\" for article in article_blocks if \"liveblog\" not in article['href']]\n",
    "        \n",
    "        print(f\"{category:>16} |\",end=\"\")\n",
    "        for link in article_links:\n",
    "            soup = soupify(link)\n",
    "            extract(soup, save_file=True)\n",
    "            print(\"=\",end=\"\")\n",
    "        print(\"|\")\n",
    "    print()\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
